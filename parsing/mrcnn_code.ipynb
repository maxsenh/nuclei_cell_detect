{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrcnn imported\n",
      "everything imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as lines\n",
    "from matplotlib.patches import Polygon\n",
    "import imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "#ROOT_DIR = \"/data/proj/smFISH/Students/Max_Senftleben/files\"\n",
    "ROOT_DIR = \"/home/max/mrcnn_b_work\"\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.visualize import display_images\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn.model import log\n",
    "print('mrcnn imported')\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "import urllib.request as down\n",
    "\n",
    "# needed for visualization\n",
    "from mrcnn.visualize import random_colors,apply_mask,find_contours\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "print('everything imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check CUDA version and GPUs\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleiConfig(Config):\n",
    "    \"\"\"Configuration for training on the nucleus segmentation dataset.\"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"nuclei\"\n",
    "\n",
    "    # Define the number of GPU to use\n",
    "    GPU_COUNT = 4\n",
    "    \n",
    "    # Adjust depending on your GPU memory\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # Background + nuclei + undefined + clusters\n",
    "\n",
    "    # Number of training and validation steps per epoch\n",
    "#     STEPS_PER_EPOCH = (300 // IMAGES_PER_GPU)\n",
    "#     VALIDATION_STEPS = max(1, 300 // IMAGES_PER_GPU)\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between nucleus and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Input image resizing\n",
    "    # Random crops of size 512x512\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 1024\n",
    "    IMAGE_MIN_SCALE = 0.5\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128,526)\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000\n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "\n",
    "    # Image mean (RGB)\n",
    "    # TODO: check if it needs to be RGB \n",
    "    MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 200\n",
    "\n",
    "    # Max number of final detections per image\n",
    "    DETECTION_MAX_INSTANCES = 400\n",
    "\n",
    "\n",
    "class NucleiInferenceConfig(NucleiConfig):\n",
    "    # Set batch size to 1 to run one image at a time\n",
    "    \n",
    "    # GPU_COUNT has to be one no????\n",
    "    \n",
    "    GPU_COUNT = 4\n",
    "    IMAGES_PER_GPU = 1\n",
    "    # Don't resize imager for inferencing\n",
    "    IMAGE_RESIZE_MODE = \"pad64\"\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NucleiDataset(utils.Dataset):    \n",
    "    \n",
    "    def add_segmentation_classes(self,labels_dict):\n",
    "        \"\"\"\n",
    "        add the classes to the dataset according to the labels\n",
    "        used for segmentation\n",
    "        \n",
    "        labels_dict: dictionary with labels and corresponding int\n",
    "    \n",
    "        \"\"\"\n",
    "        # Start class labelling from 1 because 0 is for the background\n",
    "        \n",
    "        for lab, lab_idx in labels_dict.items():\n",
    "            self.add_class(lab,lab_idx,lab)\n",
    "            \n",
    "    \n",
    "    def load_nuclei(self, dataset_dir, subset_dir):\n",
    "        \"\"\"Load a subset of the nuclei dataset.\n",
    "\n",
    "        dataset_dir: Root directory of the dataset\n",
    "        subset: Subset to load. Either the name of the sub-directory,\n",
    "                such as stage1_train, stage1_test, ...etc. or, one of:\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: validation images from VAL_IMAGE_IDS\n",
    "        \"\"\"\n",
    "   \n",
    "        \n",
    "        dataset_dir = os.path.join(dataset_dir, subset_dir)        \n",
    "        \n",
    "        # Get image ids from names of files\n",
    "        \n",
    "        image_paths = glob.glob(dataset_dir+'*.npy')\n",
    "        \n",
    "        for image_path in image_paths:\n",
    "            \n",
    "            # Define the image path\n",
    "            image_id = image_path.split('/')[-1]\n",
    "            \n",
    "            # Determine the size of the image\n",
    "            image = np.load(image_path)\n",
    "            image = skimage.util.img_as_ubyte(image)\n",
    "            image = skimage.color.grey2rgb(image)\n",
    "            height, width = image.shape[:2]\n",
    "        \n",
    "            # Add image\n",
    "            self.add_image(\n",
    "                \"nuclei\",\n",
    "                image_id=image_id,\n",
    "                path=image_path,\n",
    "                width=width, height=height)\n",
    "            \n",
    "            \n",
    "    def load_nuclei_validation(self, dataset_dir, subset_dir):\n",
    "        \"\"\"Load a subset of the nuclei dataset.\n",
    "\n",
    "        dataset_dir: Root directory of the dataset\n",
    "        subset: Subset to load. Either the name of the sub-directory,\n",
    "                such as stage1_train, stage1_test, ...etc. or, one of:\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: validation images from VAL_IMAGE_IDS\n",
    "        \"\"\"\n",
    "   \n",
    "        \n",
    "        files_dir = os.path.join(dataset_dir, subset_dir)        \n",
    "        \n",
    "        for image_id in next(os.walk(files_dir))[2]:\n",
    "            if not image_id[0:2]=='._':\n",
    "\n",
    "                # Define the image path\n",
    "                image_path = files_dir+image_id\n",
    "\n",
    "                # Determine the size of the image\n",
    "                image = skimage.io.imread(image_path)\n",
    "                image = skimage.util.img_as_ubyte(image)\n",
    "                image = skimage.color.grey2rgb(image)\n",
    "                height, width = image.shape[:2]\n",
    "\n",
    "                # Add image\n",
    "                self.add_image(\n",
    "                    \"nuclei\",\n",
    "                    image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    width=width, height=height)  \n",
    "            \n",
    "    \n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        image_info = self.image_info[image_id]\n",
    "        \n",
    "        # Load mask\n",
    "        image_path = image_info['path']\n",
    "        mask = np.load(image_path)\n",
    "        mask = mask[:,:,1:]\n",
    "        \n",
    "        class_ids = np.amax(mask,axis=(0,1)).astype(np.int32)\n",
    "        \n",
    "        return mask, class_ids\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_info = self.image_info[image_id]\n",
    "        image_path = image_info['path']\n",
    "        image = np.load(image_path)\n",
    "        image = image[:,:,0]\n",
    "        image = skimage.color.grey2rgb(image)\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    def load_image_png(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        \n",
    "        image_info = self.image_info[image_id]\n",
    "        image_path = image_info['path']\n",
    "        image = skimage.io.imread(image_path)\n",
    "        image = skimage.img_as_ubyte(image)\n",
    "        image = skimage.color.grey2rgb(image)\n",
    "        return image\n",
    "    \n",
    "    \n",
    "    def plot_masks(self,image, image_id, mask, class_ids):\n",
    "        unique_class_ids = np.unique(class_ids)\n",
    "        fig, axarr = plt.subplots(1,len(unique_class_ids)+1)\n",
    "        fig.set_size_inches((8,5))\n",
    "        axarr[0].axis('off')\n",
    "        axarr[0].imshow(image)\n",
    "        axarr[0].set_title(self.image_info[image_id]['id'])\n",
    "\n",
    "        for idx,class_id in enumerate(unique_class_ids):\n",
    "            # Pull all the labels together\n",
    "            m = mask[:, :, np.where(class_ids == class_id)[0]]\n",
    "            m = np.sum(m * np.arange(1, m.shape[-1] + 1), -1)\n",
    "            axarr[idx+1].axis('off')\n",
    "            axarr[idx+1].imshow(m)\n",
    "            \n",
    "            lab = self.class_names[class_id]\n",
    "            axarr[idx+1].set_title(lab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json file with segmented info and link to the cloud storage\n",
    "# with the images\n",
    "\n",
    "def labelbox_annotations_parser(annotations_dir,annotation_file,img_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remember that for labelbox:\n",
    "    - origin (0,0) of the reference system in bottom left corner like\n",
    "    cartesian system\n",
    "    - labelbox can label objects outside the image so the coords\n",
    "    need to be trimmed to the image size\n",
    "    - This parser consider that all the images are square and same image size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    annotations = json.load(open(annotations_dir+annotation_file))\n",
    "    \n",
    "    # Parse the annotation of the images done using labelbox.io\n",
    "    # into a better formatted dict\n",
    "    # Check with the company  where is the origin\n",
    "\n",
    "    # create incremental number for all the identified objects\n",
    "    obj_idx = 0\n",
    "\n",
    "\n",
    "    # create a labels list\n",
    "    labels_list = [] \n",
    "    labels_dict = {}\n",
    "\n",
    "    # Reformatted dict\n",
    "    annotations_dict = {}\n",
    "\n",
    "\n",
    "    # Masks reference\n",
    "    masks_urls_dict = {}\n",
    "\n",
    "    # Loop through all the images\n",
    "    # TODO: Check to skip non annotated images \n",
    "    for annotation_data in annotations:\n",
    "\n",
    "        # Avoid error when empty images\n",
    "        if isinstance(annotation_data['Label'],dict):\n",
    "        # save the image id\n",
    "            img_id = annotation_data['External ID']\n",
    "            annotations_dict[img_id] = {}\n",
    "            masks_urls_dict[img_id] = {}\n",
    "\n",
    "\n",
    "            # create subdict with labels\n",
    "            for lab in annotation_data['Label'].keys():\n",
    "                if lab not in labels_list:\n",
    "                    labels_list.append(lab) \n",
    "                annotations_dict[img_id][lab] = {}\n",
    "\n",
    "            # Create labels dict with reference number starting from 1\n",
    "            # because zero is for background\n",
    "            lab_idx = 1\n",
    "            for lab in labels_list:\n",
    "                labels_dict[lab] = lab_idx\n",
    "                lab_idx += 1\n",
    "\n",
    "            # loop through the labels and convert the coords in a list of tuples\n",
    "            for lab, objs in annotation_data['Label'].items():\n",
    "                for obj_coords in objs:\n",
    "                    coords = [tuple(coord_pair.values()) for coord_pair in obj_coords]\n",
    "                \n",
    "                    # Convert the coords in numpy array\n",
    "                    coords = np.array(coords)\n",
    "                    # labelbox reference system has (0,0) on the bottom left corner\n",
    "                    # need to correct in order to have skimage system\n",
    "                \n",
    "                    # Flip cartesian y-axis\n",
    "                    coords = np.abs(coords-np.array([0,img_size]))\n",
    "                \n",
    "                    # Swap x,y converting to [rr,cc] system\n",
    "                    coords[:,[0,1]] = coords[:,[1,0]]\n",
    "                \n",
    "                    # Remove values that are ouside the range\n",
    "                    #Remove points below 0\n",
    "                    coords[coords<0] = 0\n",
    "                \n",
    "                    # Remove the point out of the upper limit of the edges\n",
    "                    coords[coords>img_size] = img_size\n",
    "                \n",
    "                \n",
    "                    annotations_dict[img_id][lab][obj_idx] = coords\n",
    "                    obj_idx +=1\n",
    "\n",
    "        \n",
    "#        # Collect all the url for the masks\n",
    "#        for lab in annotation_data['Masks'].keys():\n",
    "#            masks_urls_dict[img_id][lab] = annotation_data['Masks'][lab]\n",
    "            \n",
    "    return annotations_dict, masks_urls_dict, labels_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the segmented images generated by labelbox if needed\n",
    "\n",
    "def labelbox_dowload_masks(saving_dir,masks_urls_dict):\n",
    "    \n",
    "    for img_id,url_dict in masks_urls_dict.items():\n",
    "        for lab,mask_url in url_dict.items():\n",
    "            reading_file = requests.get(mask_url, stream=True)\n",
    "            local_filename = saving_dir+'mask_'+lab+'_'+img_id\n",
    "            open(local_filename, 'wb').write(reading_file.content)\n",
    "\n",
    "            \n",
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# The function can be parallelized\n",
    "def chunking_labeled_images(number_chunks_dimension,class_names,dataset_dir,training_dir,chunked_dir,annotations_dict):\n",
    "\n",
    "    # Build dataset directory\n",
    "    dataset_dir = os.path.join(dataset_dir, training_dir)   \n",
    "\n",
    "    # Build chunked images directory\n",
    "    save_chunk_dir = os.path.join(dataset_dir, chunked_dir) \n",
    "\n",
    "\n",
    "    # Get image ids from annotations\n",
    "    image_ids = list(annotations_dict.keys())\n",
    "\n",
    "    for image_id in image_ids:\n",
    "\n",
    "        # Define the image path\n",
    "        image_path = dataset_dir+image_id\n",
    "\n",
    "        # segmented regions\n",
    "        polygons = annotations_dict[image_id]\n",
    "\n",
    "\n",
    "        # Determine the size of the image\n",
    "        image = skimage.io.imread(image_path)\n",
    "        \n",
    "        # Convert the image to uint8\n",
    "        image = skimage.util.img_as_ubyte(image)\n",
    "        # image = skimage.color.grey2rgb(image)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "\n",
    "        # Load and create masks array\n",
    "\n",
    "        # Determine how many objects of the different classes have been segmented\n",
    "        instance_count = 0\n",
    "        for lab in polygons:\n",
    "            instance_count += len(list(polygons[lab].values()))\n",
    "\n",
    "        mask = np.zeros([height, width, instance_count],\n",
    "                        dtype=np.uint8)\n",
    "\n",
    "\n",
    "        class_ids = []\n",
    "        mask_layer = 0\n",
    "        for lab, pgs in polygons.items():\n",
    "\n",
    "            # Get the class id\n",
    "            class_id = class_names.index(lab)\n",
    "\n",
    "            for obj_num, coords in pgs.items():\n",
    "\n",
    "                rr, cc = skimage.draw.polygon(coords[:,0], coords[:,1])\n",
    "                mask[rr, cc, mask_layer] = class_id\n",
    "                mask_layer += 1\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "\n",
    "        class_ids = np.array(class_ids)\n",
    "\n",
    "\n",
    "        # stack image with masks\n",
    "        image_mask = np.insert(mask,0,image,axis=2)\n",
    "\n",
    "\n",
    "        # consider that we are collecting square images and only even number of cuts\n",
    "        hsplits = np.split(image_mask,number_chunks_dimension,axis=0)\n",
    "        total_images = []\n",
    "        for split in hsplits:\n",
    "                total_images.append(np.split(split,number_chunks_dimension,axis=1))\n",
    "        total_images = [img for cpl in total_images for img in cpl] \n",
    "\n",
    "        # Chunk the images and the segmented data\n",
    "        for idx,image_chunk in enumerate(total_images):\n",
    "            image_chunks_ids = []\n",
    "            mask = image_chunk != 0\n",
    "            planes_to_keep = np.flatnonzero((mask).sum(axis=(0,1)))\n",
    "            # Make sure that the image has labeled objects\n",
    "            if planes_to_keep.size:\n",
    "                image_chunk_trimmed = image_chunk[:,:,planes_to_keep]\n",
    "                image_chunk_trimmed_id = image_id.split('.')[0]+'chunk'+str(idx)\n",
    "\n",
    "                np.save(save_chunk_dir+image_chunk_trimmed_id,image_chunk_trimmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a Histogram\n",
    "def histo(filename,scores,class_ids, results_histo):\n",
    "    \n",
    "    # tuples for legend of plot\n",
    "    tuple1=[]\n",
    "    tuple2=[]\n",
    "    \n",
    "    if len(scores)>0:\n",
    "        \n",
    "        scores_cluster = []\n",
    "        scores_nuclei = []\n",
    "        scores_undefined = []\n",
    "        \n",
    "        for i in range(len(scores)):\n",
    "            if class_ids[i]==1:\n",
    "                scores_nuclei.append(scores[i])\n",
    "            if class_ids[i]==2:\n",
    "                scores_undefined.append(scores[i])\n",
    "            if class_ids[i]==3:\n",
    "                scores_cluster.append(scores[i])\n",
    "        rounded_scores=[round(i,2) for i in scores]\n",
    "        rounded_scores_n=[round(i,2) for i in scores_nuclei]\n",
    "        rounded_scores_u=[round(i,2) for i in scores_undefined]\n",
    "        rounded_scores_c=[round(i,2) for i in scores_cluster]\n",
    "        n=Counter(rounded_scores_n)\n",
    "        u=Counter(rounded_scores_u)\n",
    "        c=Counter(rounded_scores_c)\n",
    "        \n",
    "        width=(max(rounded_scores)-(min(rounded_scores)*0.9))*0.02\n",
    "        \n",
    "        plo = plt.subplot(111)\n",
    "        rec1 = plo.bar(n.keys(),n.values(), width=width)\n",
    "        rec2 = plo.bar(u.keys(),u.values(), width=width)\n",
    "        rec3 = plo.bar(c.keys(),c.values(), width=width)\n",
    "        \n",
    "        if rec1:\n",
    "            tuple1.append(rec1[0])\n",
    "            tuple2.append(\"Nr of Nuclei: \"+str(len(scores_nuclei)))\n",
    "        if rec2:\n",
    "            tuple1.append(rec2[0])\n",
    "            tuple2.append(\"Nr of Undefined: \"+str(len(scores_undefined)))\n",
    "        if rec3:\n",
    "            tuple1.append(rec3[0])\n",
    "            tuple2.append(\"Nr of Cluster: \"+str(len(scores_cluster)))\n",
    "    \n",
    "    \n",
    "        mins=min(scores)\n",
    "        maxs=max(scores)\n",
    "        #maxt=max(n.values())\n",
    "    \n",
    "    else:\n",
    "        mins=0\n",
    "        maxs=1\n",
    "        maxt=1\n",
    "        \n",
    "        \n",
    "    a,b=plt.xlim(mins*0.9,maxs*1.02)\n",
    "    \n",
    "    plt.grid(axis=\"y\",alpha=0.75)    \n",
    "    plt.xlabel(\"Confidence Score\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(filename)\n",
    "    \n",
    "\n",
    "    plt.legend(tuple(tuple1),tuple(tuple2))\n",
    "    plt.savefig(results_histo+filename[:-4]+\"distr.png\",dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nuclei': {1508: array([[1133,   94],\n",
      "       [1171,  104],\n",
      "       [1202,  113],\n",
      "       [1209,  123],\n",
      "       [1188,  139],\n",
      "       [1158,  144],\n",
      "       [1131,  134],\n",
      "       [1125,  109]]), 1509: array([[1652,  258],\n",
      "       [1677,  222],\n",
      "       [1696,  187],\n",
      "       [1728,  174],\n",
      "       [1770,  182],\n",
      "       [1779,  207],\n",
      "       [1765,  251],\n",
      "       [1752,  289],\n",
      "       [1735,  291],\n",
      "       [1706,  282],\n",
      "       [1682,  264]]), 1510: array([[1784,  594],\n",
      "       [1777,  633],\n",
      "       [1784,  669],\n",
      "       [1780,  705],\n",
      "       [1785,  741],\n",
      "       [1820,  753],\n",
      "       [1845,  761],\n",
      "       [1865,  759],\n",
      "       [1865,  714],\n",
      "       [1849,  659],\n",
      "       [1827,  604],\n",
      "       [1801,  592]])}, 'undefined': {1511: array([[1176,  147],\n",
      "       [1203,  133],\n",
      "       [1215,  122],\n",
      "       [1256,  147],\n",
      "       [1279,  175],\n",
      "       [1286,  211],\n",
      "       [1266,  224],\n",
      "       [1216,  218],\n",
      "       [1187,  192]]), 1512: array([[1312,  152],\n",
      "       [1365,  139],\n",
      "       [1403,  150],\n",
      "       [1447,  162],\n",
      "       [1498,  169],\n",
      "       [1522,  193],\n",
      "       [1475,  210],\n",
      "       [1417,  211],\n",
      "       [1368,  211],\n",
      "       [1338,  181]]), 1513: array([[1068,  255],\n",
      "       [1081,  233],\n",
      "       [1118,  232],\n",
      "       [1113,  265],\n",
      "       [1084,  273]]), 1514: array([[1753,  767],\n",
      "       [1791,  765],\n",
      "       [1827,  774],\n",
      "       [1853,  788],\n",
      "       [1844,  810],\n",
      "       [1817,  810],\n",
      "       [1777,  801],\n",
      "       [1752,  786]])}}\n"
     ]
    }
   ],
   "source": [
    "# directories with the annotations info\n",
    "ROOT_DIR = \"/home/maxsen/DEEPL/\"\n",
    "\n",
    "annotations_dir = ROOT_DIR + \"/annotation/\"\n",
    "\n",
    "\n",
    "#annotation_file = 'Nuclei_annotations_and_masks_20150530.json'\n",
    "annotation_file = 'All_labels_20181107.json'\n",
    "#annotation_file = 'Labeled_with_masks_links.json'\n",
    "file1 = 'nuclei_20190205.json'\n",
    "\n",
    "\n",
    "img_size = 2048\n",
    "# '''masks_urls_dict''' for second\n",
    "annotations_dict, masks_urls_dict, labels_dict = labelbox_annotations_parser(annotations_dir,annotation_file,img_size)\n",
    "\n",
    "# Config\n",
    "\n",
    "config = NucleiConfig()\n",
    "\n",
    "print(annotations_dict[\"Nuclei_SN_Hyb2_pos_251.png\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the files from annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(anno_file, img_dir):\n",
    "\n",
    "    train_dir = img_dir + \"/train/\"\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)   \n",
    "    \n",
    "    val_dir = img_dir + \"/val/\"\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "    \n",
    "    anno = json.load(open(anno_file))\n",
    "    counter = 0\n",
    "    for s in anno:\n",
    "        if counter <= len(anno) * 0.7:\n",
    "            print(counter)\n",
    "            name = s['External ID']\n",
    "            url = s['Labeled Data']\n",
    "            down.urlretrieve(url, train_dir + name)\n",
    "            # print('downloading %s'%(name))\n",
    "            counter += 1\n",
    "        else:\n",
    "            name = s['External ID']\n",
    "            url = s['Labeled Data']\n",
    "            down.urlretrieve(url, val_dir + name)\n",
    "            print('downloading %s'%(name))\n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxsen/DEEPL//data/nuclei_all/\n"
     ]
    }
   ],
   "source": [
    "img_dir = ROOT_DIR + '/data/nuclei_all/'\n",
    "print(img_dir)\n",
    "\n",
    "# run this\n",
    "#download_img(annotations_dir + annotation_file, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxsen/DEEPL//data/nuclei_all/train_chunk2/\n"
     ]
    }
   ],
   "source": [
    "# Number of chunks to create\n",
    "# 2x2 or 4x4 etc.....\n",
    "# it must be even\n",
    "number_chunks_dimension = 2\n",
    "class_names = ['BG', 'nuclei', 'undefined', 'clusters']\n",
    "\n",
    "# Directory wt\n",
    "dataset_dir = img_dir\n",
    "\n",
    "# Prepare training dataset\n",
    "training_dir = dataset_dir + 'train/'\n",
    "validation_dir = dataset_dir + 'val/'\n",
    "\n",
    "# Chunked_dir\n",
    "chunked_train = dataset_dir + 'train_chunk2/'\n",
    "chunked_val = dataset_dir + 'val_chunk2/'\n",
    "print(chunked_train)\n",
    "\n",
    "# run chunking\n",
    "#chunking_labeled_images(number_chunks_dimension,class_names,dataset_dir,training_dir,chunked_train,annotations_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/maxsen/DEEPL//data/nuclei_all/train_chunk2/\n",
      "/home/maxsen/DEEPL//data/nuclei_all/val_chunk2/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare training dataset\n",
    "training_dir = chunked_train\n",
    "nuclei_training = NucleiDataset()\n",
    "nuclei_training.add_segmentation_classes(labels_dict)\n",
    "nuclei_training.load_nuclei(dataset_dir, training_dir)\n",
    "nuclei_training.prepare()\n",
    "\n",
    "print(training_dir)\n",
    "\n",
    "# Prepare validating dataset\n",
    "validation_dir = chunked_val\n",
    "nuclei_validation = NucleiDataset()\n",
    "nuclei_validation.add_segmentation_classes(labels_dict)\n",
    "nuclei_validation.load_nuclei(dataset_dir, validation_dir)\n",
    "nuclei_validation.prepare()\n",
    "\n",
    "print(validation_dir)\n",
    "\n",
    "# give class names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the chunks if their masks match their objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/maxsen/DEEPL//data/nuclei_all/train_chunk2/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8281d7dcb724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlist_of_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_of_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_train\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_imgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/maxsen/DEEPL//data/nuclei_all/train_chunk2/'"
     ]
    }
   ],
   "source": [
    "# Check chunks\n",
    "\n",
    "list_of_imgs = os.listdir(chunked_train)\n",
    "\n",
    "list_of_shapes = [np.load(chunked_train + image) for image in list_of_imgs]\n",
    "\n",
    "real_list_of_shapes = set([i.shape for i in list_of_shapes])\n",
    "\n",
    "# list of images containing objects\n",
    "list_of_imgs_with_1=[]\n",
    "\n",
    "print(real_list_of_shapes)\n",
    "for i in range(len(list_of_shapes)):\n",
    "    if list_of_shapes[i].shape==(512,512,1):\n",
    "        list_of_imgs_with_1.append(list_of_shapes[i])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize chunks\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "# modified mrcnn.visualize code\n",
    "# set path for saving the tested chunks with masks\n",
    "test_chunks = training_dir + \"small_dataset_160_original_4+4\"\n",
    "\n",
    "\n",
    "def create_image(image_id,s):\n",
    "\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(nuclei_validation, config, image_id, use_mini_mask=False)\n",
    "    \n",
    "    visualize.display_instances(image,gt_bbox,gt_mask,\n",
    "                               gt_class_id,class_names)\n",
    "\n",
    "for i in range(0,50):\n",
    "    \n",
    "    image_id = random.choice(nuclei_validation.image_ids)\n",
    "    \n",
    "    # pick this if you want to run it\n",
    "    #create_image(image_id,i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/max/mrcnn_b_work/model_mat_mrcnn_nuclei /home/max/mrcnn_b_work/mask_rcnn_coco.h5\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = os.path.join(ROOT_DIR, \"model_mat_mrcnn_nuclei\")\n",
    "\n",
    "# if the model is there do nothing if not download\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "print(MODEL_DIR, COCO_MODEL_PATH)\n",
    "\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0208 16:26:32.827497 47840765419136 deprecation.py:323] From /home/max/anaconda3/envs/mat_mrcnn/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:2437: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    }
   ],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain model with coco dataset\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = iaa.Sometimes(0.9, [\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Flipud(0.5),\n",
    "    iaa.Multiply((0.8, 1.2)),\n",
    "    iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/max/mrcnn_b_work/model_mat_mrcnn_nuclei/nuclei20190208T1637/mask_rcnn_nuclei_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0208 16:39:29.899932 47840765419136 deprecation.py:323] From /home/max/anaconda3/envs/mat_mrcnn/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "/home/max/anaconda3/envs/mat_mrcnn/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/max/anaconda3/envs/mat_mrcnn/lib/python3.6/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Training of the Head branches\n",
    "\n",
    "model.train(nuclei_training, nuclei_validation, \n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            augmentation = augmentation,\n",
    "            epochs=5, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import head-trained model\n",
    "model.load_weights(model.find_last()[1], by_name=True)\n",
    "\n",
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(nuclei_training, nuclei_validation, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=5, \n",
    "            layers='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  512\n",
      "IMAGE_MIN_SCALE                0.5\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               200\n",
      "MEAN_PIXEL                     [43.53 39.56 48.22]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           nuclei\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         1000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (16, 32, 64, 128, 526)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.9\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    64\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           128\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device to load the neural network on.\n",
    "# Useful if you're training a model on the same \n",
    "# machine, in which case use CPU and leave the\n",
    "# GPU for training.\n",
    "DEVICE = \"/cpu:4\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "# Inspect the model in training or inference modes\n",
    "# values: 'inference' or 'training'\n",
    "# TODO: code for 'training' test mode not ready yet\n",
    "TEST_MODE = \"inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/maxsen/Project_mrcnn/small_dataset_trained_100epochs_gcloud/mask_rcnn_nuclei_0100.h5\n"
     ]
    }
   ],
   "source": [
    "# Create model in inference mode\n",
    "\n",
    "# use this if you were training on the same machine\n",
    "#MODEL_DIR = os.path.join(ROOT_DIR, \"model_nuclei\")\n",
    "\n",
    "# use this if you only run inference\n",
    "# specify folder of trained model to use for inference\n",
    "MODEL_DIR = ROOT_DIR + \"/small_dataset_trained_100epochs_gcloud/\"\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\n",
    "                              config=config)\n",
    "weights_path = MODEL_DIR + \"mask_rcnn_nuclei_0100.h5\"\n",
    "print(\"Loading weights from \", weights_path)\n",
    "model.load_weights(weights_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape:  [512 512   3]\n",
      "Processing 1 images\n",
      "image                    shape: (1024, 1024, 3)       min:    0.00000  max:  201.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:    0.00000  max:  201.00000  uint8\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 1024.00000  int64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "scores has incompatible shape\n\t [[{{node mrcnn_detection_1/map/while/non_max_suppression/NonMaxSuppressionV3}} = NonMaxSuppressionV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mrcnn_detection_1/map/while/GatherV2, mrcnn_detection_1/map/while/GatherV2_1, mrcnn_detection_1/map/while/sub/x, mrcnn_detection_1/map/while/non_max_suppression/iou_threshold, mrcnn_detection_1/map/while/non_max_suppression/score_threshold)]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e7f03a9f16b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_molded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepl/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\u001b[0m in \u001b[0;36mdetect_molded\u001b[0;34m(self, molded_images, image_metas, verbose)\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;31m# Run object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2581\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmrcnn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2582\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmolded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_metas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2583\u001b[0m         \u001b[0;31m# Process detections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: scores has incompatible shape\n\t [[{{node mrcnn_detection_1/map/while/non_max_suppression/NonMaxSuppressionV3}} = NonMaxSuppressionV3[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](mrcnn_detection_1/map/while/GatherV2, mrcnn_detection_1/map/while/GatherV2_1, mrcnn_detection_1/map/while/sub/x, mrcnn_detection_1/map/while/non_max_suppression/iou_threshold, mrcnn_detection_1/map/while/non_max_suppression/score_threshold)]]"
     ]
    }
   ],
   "source": [
    "image_id = random.choice(nuclei_validation.image_ids)\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(nuclei_validation, config, image_id, use_mini_mask=False)\n",
    "info = nuclei_validation.image_info[image_id]\n",
    "# print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n",
    "#                                        nuclei_validation.image_reference(image_id)))\n",
    "print(\"Original image shape: \", modellib.parse_image_meta(image_meta[np.newaxis,...])[\"original_image_shape\"][0])\n",
    "\n",
    "# Run object detection\n",
    "results = model.detect_molded(np.expand_dims(image, 0), np.expand_dims(image_meta, 0), verbose=1)\n",
    "\n",
    "# Display results\n",
    "r = results[0]\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "# Compute AP over range 0.5 to 0.95 and print it\n",
    "utils.compute_ap_range(gt_bbox, gt_class_id, gt_mask,\n",
    "                       r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "                       verbose=1)\n",
    "\n",
    "visualize.display_differences(\n",
    "    image,\n",
    "    gt_bbox, gt_class_id, gt_mask,\n",
    "    r['rois'], r['class_ids'], r['scores'], r['masks'],\n",
    "    nuclei_validation.class_names, ax=get_ax(),\n",
    "    show_box=False, show_mask=False,\n",
    "    iou_threshold=0.5, score_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment the test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data_name\n",
    "file_segmented = 'segmented_result_T1_190128.pkl'\n",
    "\n",
    "# folder to save results\n",
    "results_dir = ROOT_DIR + '/results_190128/'\n",
    "\n",
    "# datafolder to segment\n",
    "seg_dir = ROOT_DIR + '/data/training_data/Nuclei-Test_used_for_test/'\n",
    "\n",
    "nuclei_seg = NucleiDataset()\n",
    "nuclei_seg.add_segmentation_classes(labels_dict)\n",
    "nuclei_seg.load_nuclei_validation(dataset_dir, seg_dir)\n",
    "nuclei_seg.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181107_182739_066_Seq0004_4.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  192.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:  148.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_10.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  193.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:  150.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_9.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  141.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:   94.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_13.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_7.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  240.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  198.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_2.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_1.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  247.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  204.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_5.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  198.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:  155.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_6.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_8.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  219.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  174.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_3.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    1.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -47.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_15.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_14.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  234.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  189.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_11.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    4.00000  max:  255.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -44.22000  max:  215.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n",
      "20181107_182739_066_Seq0004_12.png\n",
      "Processing 1 images\n",
      "image                    shape: (2048, 2048, 3)       min:    2.00000  max:  177.00000  uint8\n",
      "molded_images            shape: (1, 1024, 1024, 3)    min:  -46.22000  max:  130.44000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max: 2048.00000  float64\n",
      "anchors                  shape: (1, 261888, 4)        min:   -0.36358  max:    1.30101  float32\n"
     ]
    }
   ],
   "source": [
    "# for multiple images\n",
    "# sometimes I receive errors...sometimes not, memory related maybe?\n",
    "\n",
    "for image_id in nuclei_seg.image_ids:\n",
    "    print(nuclei_seg.image_info[image_id]['id'])\n",
    "    fname = nuclei_seg.image_info[image_id]['id'].split('.')[0]+'_seg_results.pkl'\n",
    "    image = nuclei_seg.load_image_png(image_id)\n",
    "    image_info = nuclei_seg.image_info[image_id]\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=1)\n",
    "    pickle.dump(results,open(results_dir+fname,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 2048, 3)\n",
      "uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbe29d08b00>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_info = nuclei_seg.image_info[image_id]\n",
    "image_path = image_info['path']\n",
    "image = skimage.io.imread(image_path)\n",
    "image = skimage.img_as_ubyte(image)\n",
    "image = skimage.color.grey2rgb(image)\n",
    "print(image.shape)\n",
    "print(image.dtype)\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results and showing Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path's\n",
    "\n",
    "# path of the resulting pkl's\n",
    "results_pkl = ROOT_DIR + \"/results_190128/pkl/\"\n",
    "results_segmented = ROOT_DIR + \"/results_190128/segmented_images_masks/\"\n",
    "# creating image folder if not present\n",
    "if not os.path.exists(results_segmented):\n",
    "    os.makedirs(results_segmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification of mrcnn.visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "def display_instances(image, boxes, masks, class_ids, class_names,imagename,\n",
    "                      scores=None, title=\"\",\n",
    "                      figsize=(16, 16), ax=None,\n",
    "                      show_mask=True, show_bbox=True,\n",
    "                      colors=None, captions=None):\n",
    "    \"\"\"\n",
    "    boxes: [num_instance, (y1, x1, y2, x2, class_id)] in image coordinates.\n",
    "    masks: [height, width, num_instances]\n",
    "    class_ids: [num_instances]\n",
    "    class_names: list of class names of the dataset\n",
    "    scores: (optional) confidence scores for each box\n",
    "    title: (optional) Figure title\n",
    "    show_mask, show_bbox: To show masks and bounding boxes or not\n",
    "    figsize: (optional) the size of the image\n",
    "    colors: (optional) An array or colors to use with each object\n",
    "    captions: (optional) A list of strings to use as captions for each object\n",
    "    \"\"\"\n",
    "    # Number of instances\n",
    "    N = boxes.shape[0]\n",
    "    if not N:\n",
    "        print(\"\\n*** No instances to display *** \\n\")\n",
    "\n",
    "    # If no axis is passed, create one and automatically call show()\n",
    "    auto_show = False\n",
    "    if not ax:\n",
    "        _, ax = plt.subplots(1, figsize=figsize)\n",
    "        auto_show = True\n",
    "\n",
    "    # Generate random colors\n",
    "    colors = colors or random_colors(N)\n",
    "\n",
    "    # Show area outside image boundaries.\n",
    "    height, width = image.shape[:2]\n",
    "    ax.set_ylim(height + 10, -10)\n",
    "    ax.set_xlim(-10, width + 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    masked_image = image.astype(np.uint32).copy()\n",
    "    \n",
    "    for i in range(N):\n",
    "        color = colors[i]\n",
    "\n",
    "        # Bounding box\n",
    "        if not np.any(boxes[i]):\n",
    "            # Skip this instance. Has no bbox. Likely lost in image cropping.\n",
    "            continue\n",
    "        y1, x1, y2, x2 = boxes[i]\n",
    "        if show_bbox:\n",
    "            p = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2,\n",
    "                                alpha=0.7, linestyle=\"dashed\",\n",
    "                                edgecolor=color, facecolor='none')\n",
    "            ax.add_patch(p)\n",
    "\n",
    "        # Label\n",
    "        if not captions:\n",
    "            \n",
    "            class_id = class_ids[i]\n",
    "            \n",
    "            score = scores[i] if scores is not None else None\n",
    "            label = class_names[class_id]\n",
    "            x = random.randint(x1, (x1 + x2) // 2)\n",
    "            caption = \"{} {:.3f}\".format(label, score) if score else label\n",
    "        else:\n",
    "            caption = captions[i]\n",
    "        ax.text(x1, y1 + 8, caption,\n",
    "                color='w', size=11, backgroundcolor=\"none\")\n",
    "\n",
    "        # Mask\n",
    "        mask = masks[:, :, i]\n",
    "        if show_mask:\n",
    "            masked_image = apply_mask(masked_image, mask, color)\n",
    "\n",
    "        # Mask Polygon\n",
    "        # Pad to ensure proper polygons for masks that touch image edges.\n",
    "        padded_mask = np.zeros(\n",
    "            (mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            #Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=color)\n",
    "            ax.add_patch(p)\n",
    "    ax.imshow(masked_image.astype(np.uint8))\n",
    "    \n",
    "    print(imagename)\n",
    "    plt.gca().set_axis_off()\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "    plt.margins(0,0)\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "        \n",
    "    plt.savefig(results_segmented+imagename+\".png\",bbox = \"tight\", pad_inches = 0)\n",
    "    plt.close()\n",
    "    \n",
    "    # what does this do?\n",
    "    if auto_show:\n",
    "        plt.savefig(results_segmented+imagename+\".png\",bbox = \"tight\", pad_inches = 0)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Images plus Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "20181107_182739_066_Seq0004_1_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_1_seg_results\n",
      "1\n",
      "20181107_182739_066_Seq0004_2_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_2_seg_results\n",
      "2\n",
      "20181107_182739_066_Seq0004_3_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_3_seg_results\n",
      "3\n",
      "20181107_182739_066_Seq0004_4_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_4_seg_results\n",
      "4\n",
      "20181107_182739_066_Seq0004_5_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_5_seg_results\n",
      "5\n",
      "20181107_182739_066_Seq0004_6_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_6_seg_results\n",
      "6\n",
      "20181107_182739_066_Seq0004_7_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_7_seg_results\n",
      "7\n",
      "20181107_182739_066_Seq0004_8_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_8_seg_results\n",
      "8\n",
      "20181107_182739_066_Seq0004_9_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_9_seg_results\n",
      "9\n",
      "20181107_182739_066_Seq0004_10_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_10_seg_results\n",
      "10\n",
      "20181107_182739_066_Seq0004_11_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_11_seg_results\n",
      "11\n",
      "20181107_182739_066_Seq0004_12_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_12_seg_results\n",
      "12\n",
      "20181107_182739_066_Seq0004_13_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_13_seg_results\n",
      "13\n",
      "20181107_182739_066_Seq0004_14_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_14_seg_results\n",
      "14\n",
      "20181107_182739_066_Seq0004_15_seg_results.pkl\n",
      "20181107_182739_066_Seq0004_15_seg_results\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "from mrcnn.visualize import random_colors,apply_mask,find_contours\n",
    "from collections import Counter\n",
    "\n",
    "# list of imagenames\n",
    "list_of_images = []\n",
    "\n",
    "# list of scores\n",
    "list_scores = []\n",
    "\n",
    "# list of db objects\n",
    "list_class_objects=[]\n",
    "\n",
    "d=0\n",
    "pkl_lis=os.listdir(results_pkl)\n",
    "print(nuclei_seg.image_ids)\n",
    "for image_id in range(1,16):\n",
    "    if d<200:\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    image = skimage.io.imread(seg_dir + \"/20181107_182739_066_Seq0004_\"+str(image_id)+\".png\")\n",
    "    image = skimage.img_as_ubyte(image)\n",
    "    image = skimage.color.grey2rgb(image)\n",
    "    segfilename = \"20181107_182739_066_Seq0004_\"+str(image_id)+\"_seg_results.pkl\"\n",
    "    print(segfilename)\n",
    "    #segfilename = nuclei_seg.image_info[image_id]['id'].split('.')[0]+'_seg_results.pkl'\n",
    "\n",
    "    results = pickle.load(open(results_pkl+segfilename,'rb'))\n",
    "    r = results[0]\n",
    "    \n",
    "    display_instances(\n",
    "        image,\n",
    "        r['rois'], r['masks'], r['class_ids'], nuclei_seg.class_names,segfilename[:-4],\n",
    "        ax=get_ax(),\n",
    "        show_bbox=True, show_mask=True)\n",
    "\n",
    "    d+=1\n",
    "    list_of_images.append(segfilename)\n",
    "    list_scores.append(r['scores'])\n",
    "    list_class_objects.append(r['class_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compute Histograms showing the confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path for histograms\n",
    "results_histo = ROOT_DIR + \"/results_190128/histograms/\"\n",
    "if not os.path.exists(results_histo):\n",
    "    os.makedirs(results_histo)\n",
    "\n",
    "for i in range(len(list_of_images)):\n",
    "    histo(list_of_images[i],list_scores[i],list_class_objects[i], results_histo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make results PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.325\n"
     ]
    }
   ],
   "source": [
    "dir_pic = results_segmented\n",
    "dir_his = results_histo \n",
    "dir_unseg = seg_dir\n",
    "dataset_segm=\"model_original_gcloud_epoch100_withunseg\"\n",
    "\n",
    "import os\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter,A4\n",
    "from reportlab.platypus import SimpleDocTemplate,Table,PageBreak, Paragraph, Spacer, Image\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.lib.enums import TA_RIGHT\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.styles import ParagraphStyle\n",
    "#o\n",
    "#\n",
    "\n",
    "styles = getSampleStyleSheet()\n",
    "style_right = ParagraphStyle(name='right', parent=styles['Normal'], alignment=TA_RIGHT)\n",
    "doc = SimpleDocTemplate(ROOT_DIR + \"/Eval.pdf\", topMargin=10)\n",
    "Story = []\n",
    "\n",
    "# specify title\n",
    "Story.append(Paragraph(\"Training on small, testing with Nuclei-Test\", styles['Title']))\n",
    "\n",
    "a=2.3\n",
    "b=3.1\n",
    "c=b*(3/4)\n",
    "print(c)\n",
    "\n",
    "counter=0\n",
    "for i in range(1,12):\n",
    "    his=os.listdir(dir_his)\n",
    "    unseg=os.listdir(dir_unseg)\n",
    "    pic=os.listdir(dir_pic)\n",
    "    if counter<2:\n",
    "        pass\n",
    "    else:\n",
    "        Story.append(PageBreak())\n",
    "        \n",
    "    tb_data = [[\n",
    "                Image(dir_unseg+\"20181107_182739_066_Seq0004_\"+str(i)+\".png\",a*inch, a*inch,hAlign=\"RIGHT\"),\n",
    "                Image(dir_pic+\"20181107_182739_066_Seq0004_\"+str(i)+\"_seg_results.png\",   a*inch, a*inch,hAlign=\"RIGHT\"),\n",
    "                Image(dir_his+\"20181107_182739_066_Seq0004_\"+str(i)+\"_seg_resultsdistr.png\" , b*inch,c*inch, hAlign=\"LEFT\")]\n",
    "    ]\n",
    "    tb = Table(tb_data)\n",
    "    Story.append(tb)\n",
    "    counter=+1\n",
    "    \n",
    "\n",
    "# write into doc\n",
    "doc.build(Story)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "deep_learning",
=======
   "display_name": "pytorch_maskrcnn",
>>>>>>> 8bbb620560dded3d67a170304c683335f90a26ba
   "language": "python",
   "name": "pt_mask_rcnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
