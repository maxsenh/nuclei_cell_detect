{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggesting maskrcnn-benchmark is installed with the correct environment and GCC version (see bottom) the following explains how to run Mask R-CNN benchmark with annotations from Labelbox."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to use your own data from Labelbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- export your annotation file from labelbox in JSON WKT format\n",
    "- split annotation in training (3/5), validation (1/5) and test (1/5)\n",
    "- download the images and split them directly in train, val and test\n",
    "- see script\n",
    "\n",
    "IMPORTANT: labelbox2coco has to be installed before with pip install LBExporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import labelbox2coco as lb2co\n",
    "import urllib.request as down\n",
    "import os\n",
    "\n",
    "# ROOT on Monod, folder to save data\n",
    "DATA = '/data/proj/smFISH/Students/Max_Senftleben/files'\n",
    "ROOT_DIR = '/home/max/mrcnn_b_work'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotation(labeled_data):\n",
    "    \n",
    "    coco_output = labeled_data[:-5] + \"_coco.json\"\n",
    "    lb2co.from_json(labeled_data = labeled_data, coco_output=coco_output)\n",
    "    \n",
    "    # check if temp file exists\n",
    "    if os.path.isfile(coco_output) == True: \n",
    "        print(\"remove temp file: \", labeled_data)\n",
    "        os.remove(labeled_data)\n",
    "    else:\n",
    "        print(\"file was not converted\")\n",
    "        \n",
    "def handle_annotation(anno_dir, lbx_json, image_dir):\n",
    "    \n",
    "    # make split folder for images\n",
    "    train_dir = image_dir + \"/train/\"\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "\n",
    "    val_dir = image_dir + \"/val/\"\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "        \n",
    "    test_dir = image_dir + \"/test/\"\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.makedirs(test_dir)\n",
    "    \n",
    "    # split 3/5 train, 1/5 val and 1/5 test\n",
    "    data = json.load(open(anno_dir + lbx_json))\n",
    "    anno_train = []\n",
    "    anno_val = []\n",
    "    anno_test = []\n",
    "    counter = 0\n",
    "    for one_img in data:\n",
    "        \n",
    "        # training set 3/5\n",
    "        if counter <= len(data) * 0.6:\n",
    "            \n",
    "            # download image\n",
    "            img_id = one_img[\"External ID\"]\n",
    "            name_ = train_dir + img_id\n",
    "            down.urlretrieve(one_img[\"Labeled Data\"], name_)\n",
    "            print(\"downloading file \", name_)\n",
    "            \n",
    "            # append to json list\n",
    "            anno_train.append(one_img)\n",
    "            counter += 1\n",
    "            \n",
    "        # validation set 1/5\n",
    "        elif counter <= len(data) * 0.8:\n",
    "            \n",
    "            # download image\n",
    "            img_id = one_img[\"External ID\"]\n",
    "            name_ = val_dir + img_id\n",
    "            down.urlretrieve(one_img[\"Labeled Data\"], name_)\n",
    "            print(\"downloading file \", name_)\n",
    "            \n",
    "            # append to json list\n",
    "            anno_val.append(one_img)\n",
    "            counter += 1\n",
    "           \n",
    "        # test set 1/5\n",
    "        else:\n",
    "            \n",
    "            # download image\n",
    "            img_id = one_img[\"External ID\"]\n",
    "            name_ = test_dir + img_id\n",
    "            down.urlretrieve(one_img[\"Labeled Data\"], name_)\n",
    "            print(\"downloading file \", name_)\n",
    "            \n",
    "            # append to json list\n",
    "            anno_test.append(one_img)\n",
    "            counter += 1\n",
    "    \n",
    "    temp_train = anno_dir + \"/train.json\"\n",
    "    with open(temp_train, \"w\") as wr:\n",
    "        json.dump(anno_train,wr, separators=(',', ':'))\n",
    "        print(\"convert \", temp_train)\n",
    "        \n",
    "    convert_annotation(temp_train)\n",
    "\n",
    "    temp_val = anno_dir + \"/val.json\"\n",
    "    with open(temp_val, \"w\") as wr:\n",
    "        json.dump(anno_val, wr, separators=(',', ':'))\n",
    "        print(\"convert \", temp_val)\n",
    "        \n",
    "    convert_annotation(temp_val)\n",
    "    \n",
    "    temp_test = anno_dir + \"/test.json\"\n",
    "    with open(temp_test, \"w\") as wr:\n",
    "        json.dump(anno_test, wr, separators=(',', ':'))\n",
    "        print(\"convert \", temp_test)\n",
    "        \n",
    "    convert_annotation(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify filenames\n",
    "\n",
    "# Folder for Annotation files to save\n",
    "# Examples:\n",
    "\n",
    "ANNO = \"/home/maxsen/git/master_thesis/data/annotations/new_nuclei\"\n",
    "#anno_dir = ROOT_DIR + \"/annotation/new_nuclei_mask\"\n",
    "\n",
    "\n",
    "# Raw Annotation file from labelbox\n",
    "# Examples:\n",
    "\n",
    "annotation_file = \"/nuclei_20190205.json\"\n",
    "#annotation_file = \"/nuclei_20190205_with_masks.json\"\n",
    "\n",
    "# Folder for saving the Images\n",
    "# Examples:\n",
    "\n",
    "img_dir = \"/home/maxsen/DEEPL/data/nuclei_20190205_data\"\n",
    "#img_dir = DATA + \"/data/self_label\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below will download the images in the specified folder and will create there three folder /train, /val and /test. In the folder of the annotation file, it will create three annotation files, train_coco.json, val_coco.json and test_coco.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_144.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_195.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_106.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_196.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_107.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_7.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_199.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_108.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_8.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_200.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_203.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_109.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_110.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_11.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_206.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_116.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_13.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_208.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_118.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_15.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_211.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_120.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_16.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_212.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_124.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_18.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_213.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_125.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_20.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_214.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_126.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_23.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_127.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_215.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_28.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_217.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_129.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_29.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_218.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_131.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_31.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_219.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_133.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_32.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_221.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_134.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_33.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_228.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_136.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_36.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_231.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_138.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_37.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_232.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_38.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_139.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_233.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_141.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_43.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_237.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_143.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_44.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_238.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_2.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_45.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_239.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_149.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_48.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_240.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_153.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_50.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_241.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_155.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_51.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_249.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_156.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/train/Nuclei_SN_Hyb2_pos_53.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_251.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_55.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_158.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_254.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_159.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_56.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_160.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_58.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_59.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_161.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_60.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_162.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_62.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_163.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_67.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_165.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_166.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_71.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_73.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_167.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_76.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_169.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_170.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_77.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_79.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/val/Nuclei_SN_Hyb2_pos_171.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_172.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_84.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_173.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_86.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_177.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_88.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_89.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_178.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_93.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_181.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_182.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_94.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_95.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_186.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_96.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_188.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_98.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_190.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_99.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_191.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_192.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_100.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_194.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_SN_Hyb2_pos_102.png\n",
      "downloading file  /home/maxsen/DEEPL/data/nuclei_20190205_data/test/Nuclei_RNA_60X_pos_35.png\n",
      "convert  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/train.json\n",
      "remove temp file:  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/train.json\n",
      "convert  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/val.json\n",
      "remove temp file:  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/val.json\n",
      "convert  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/test.json\n",
      "remove temp file:  /home/maxsen/git/master_thesis/data/annotations/new_nuclei/test.json\n"
     ]
    }
   ],
   "source": [
    "handle_annotation(ANNO, annotation_file, img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rewrite IDs in the annotation files so that they are fully in COCO style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is a bit tricky as it depends on your data files. COCO annotation files have in this case 5 sub-classes, namely images, annotations, licenses, categories and info. In COCO[\"images\"] the [\"id\"] have to be changed to an integer referring to the correct file name and [\"file_name\"] has to be changed to the correct filename such as \"filename_1234.jpg\", where 1234 is the integer ID. In COCO[\"annotations\"] the [\"image_id\"] has to be changed to the integer referring to the image file, in this case 1234. This is done in the following script, where one has to provide the annotation file to be changed. This has to be done one time for each the training, validation and testing annotation file. The new annotation file will be named for example train_coco_id.json.\n",
    "\n",
    "In this case, the script assumes that the filename is a long url with its original filename something like \"http://url-to-file/Nuclei_SN_Hyb2_pos_106.png8589173798579\". The script then assigns \"Nuclei_SN_Hyb2_pos_106.png\" as the file name and \"106\" as the integer ID. By using other images, the script below may be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_ids_nuclei(annotation_file):\n",
    "    \n",
    "    anno_dir = json.load(open(annotation_file))\n",
    "    \n",
    "    for i in range(len(anno_dir['images'])):\n",
    "        one_element = anno_dir['images'][i]\n",
    "        \n",
    "        # get name of file\n",
    "        index_1 = one_element['file_name'].find('Nuclei')\n",
    "        index_2 = one_element['file_name'].find('.png')\n",
    "        correct_name_for_id = one_element['file_name'][index_1:index_2]\n",
    "        print(correct_name_for_id)\n",
    "        \n",
    "        # remove URL in filename\n",
    "        anno_dir['images'][i]['file_name'] = correct_name_for_id + '.png'\n",
    "        \n",
    "        # keep wrong ID to get data from 'annotations'\n",
    "        wrong_name_for_id = anno_dir['images'][i]['id']\n",
    "        \n",
    "        # change ID to correct with type int\n",
    "        anno_dir['images'][i]['id'] = int(correct_name_for_id[19:])\n",
    "        \n",
    "        for i in range(len(anno_dir['annotations'])):\n",
    "            if anno_dir['annotations'][i]['image_id'] == wrong_name_for_id:\n",
    "                anno_dir['annotations'][i]['image_id'] = int(correct_name_for_id[19:])\n",
    "                \n",
    "\n",
    "    with open(annotation_file[:-5] + '_id.json', \"w\") as wr:\n",
    "        json.dump(anno_dir, wr, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclei_SN_Hyb2_pos_144\n",
      "Nuclei_SN_Hyb2_pos_195\n",
      "Nuclei_SN_Hyb2_pos_106\n",
      "Nuclei_SN_Hyb2_pos_196\n",
      "Nuclei_SN_Hyb2_pos_107\n",
      "Nuclei_SN_Hyb2_pos_7\n",
      "Nuclei_SN_Hyb2_pos_199\n",
      "Nuclei_SN_Hyb2_pos_108\n",
      "Nuclei_SN_Hyb2_pos_8\n",
      "Nuclei_SN_Hyb2_pos_200\n",
      "Nuclei_SN_Hyb2_pos_203\n",
      "Nuclei_SN_Hyb2_pos_109\n",
      "Nuclei_SN_Hyb2_pos_110\n",
      "Nuclei_SN_Hyb2_pos_11\n",
      "Nuclei_SN_Hyb2_pos_206\n",
      "Nuclei_SN_Hyb2_pos_116\n",
      "Nuclei_SN_Hyb2_pos_13\n",
      "Nuclei_SN_Hyb2_pos_208\n",
      "Nuclei_SN_Hyb2_pos_118\n",
      "Nuclei_SN_Hyb2_pos_15\n",
      "Nuclei_SN_Hyb2_pos_211\n",
      "Nuclei_SN_Hyb2_pos_120\n",
      "Nuclei_SN_Hyb2_pos_16\n",
      "Nuclei_SN_Hyb2_pos_212\n",
      "Nuclei_SN_Hyb2_pos_124\n",
      "Nuclei_SN_Hyb2_pos_18\n",
      "Nuclei_SN_Hyb2_pos_213\n",
      "Nuclei_SN_Hyb2_pos_125\n",
      "Nuclei_SN_Hyb2_pos_20\n",
      "Nuclei_SN_Hyb2_pos_214\n",
      "Nuclei_SN_Hyb2_pos_126\n",
      "Nuclei_SN_Hyb2_pos_23\n",
      "Nuclei_SN_Hyb2_pos_127\n",
      "Nuclei_SN_Hyb2_pos_215\n",
      "Nuclei_SN_Hyb2_pos_28\n",
      "Nuclei_SN_Hyb2_pos_217\n",
      "Nuclei_SN_Hyb2_pos_129\n",
      "Nuclei_SN_Hyb2_pos_29\n",
      "Nuclei_SN_Hyb2_pos_218\n",
      "Nuclei_SN_Hyb2_pos_131\n",
      "Nuclei_SN_Hyb2_pos_31\n",
      "Nuclei_SN_Hyb2_pos_219\n",
      "Nuclei_SN_Hyb2_pos_133\n",
      "Nuclei_SN_Hyb2_pos_32\n",
      "Nuclei_SN_Hyb2_pos_221\n",
      "Nuclei_SN_Hyb2_pos_134\n",
      "Nuclei_SN_Hyb2_pos_33\n",
      "Nuclei_SN_Hyb2_pos_228\n",
      "Nuclei_SN_Hyb2_pos_136\n",
      "Nuclei_SN_Hyb2_pos_36\n",
      "Nuclei_SN_Hyb2_pos_231\n",
      "Nuclei_SN_Hyb2_pos_138\n",
      "Nuclei_SN_Hyb2_pos_37\n",
      "Nuclei_SN_Hyb2_pos_232\n",
      "Nuclei_SN_Hyb2_pos_38\n",
      "Nuclei_SN_Hyb2_pos_139\n",
      "Nuclei_SN_Hyb2_pos_233\n",
      "Nuclei_SN_Hyb2_pos_141\n",
      "Nuclei_SN_Hyb2_pos_43\n",
      "Nuclei_SN_Hyb2_pos_237\n",
      "Nuclei_SN_Hyb2_pos_143\n",
      "Nuclei_SN_Hyb2_pos_44\n",
      "Nuclei_SN_Hyb2_pos_238\n",
      "Nuclei_SN_Hyb2_pos_2\n",
      "Nuclei_SN_Hyb2_pos_45\n",
      "Nuclei_SN_Hyb2_pos_239\n",
      "Nuclei_SN_Hyb2_pos_149\n",
      "Nuclei_SN_Hyb2_pos_48\n",
      "Nuclei_SN_Hyb2_pos_240\n",
      "Nuclei_SN_Hyb2_pos_153\n",
      "Nuclei_SN_Hyb2_pos_50\n",
      "Nuclei_SN_Hyb2_pos_241\n",
      "Nuclei_SN_Hyb2_pos_155\n",
      "Nuclei_SN_Hyb2_pos_51\n",
      "Nuclei_SN_Hyb2_pos_249\n",
      "Nuclei_SN_Hyb2_pos_156\n",
      "Nuclei_SN_Hyb2_pos_53\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "rewrite_ids_nuclei(ANNO + '/train_coco.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configure paths_catalog.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths of the image data and the annotation data have to be added to the maskrcnn-benchmark/maskrcnn_benchmark/config/paths_catalog.py file like this:\n",
    "\n",
    "    \"coco_nuclei_train\": { \n",
    "            \"img_dir\": \"/data/proj/smFISH/Students/Max_Senftleben/files/data/nuclei_20190205_data/train\",\n",
    "            \"ann_file\":\t\"/data/proj/smFISH/Students/Max_Senftleben/files/annotation/new_nuclei/train_coco_id.json\"\n",
    "        },\n",
    "    \"coco_nuclei_val\": { \n",
    "            \"img_dir\": \"/data/proj/smFISH/Students/Max_Senftleben/files/data/nuclei_20190205_data/val\",\n",
    "            \"ann_file\":\t\"/data/proj/smFISH/Students/Max_Senftleben/files/annotation/new_nuclei/val_coco_id.json\"\n",
    "        },\n",
    "    \"coco_nuclei_test\": { \n",
    "            \"img_dir\": \"/data/proj/smFISH/Students/Max_Senftleben/files/data/nuclei_20190205_data/test\",\n",
    "            \"ann_file\":\t\"/data/proj/smFISH/Students/Max_Senftleben/files/annotation/new_nuclei/test_coco_id.json\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Configure existing or make own .yaml configuration file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing config files can be found in maskrcnn-benchmark/config/, datasets have to be specified at DATASETS according to the keyword set in the paths_catalog.py file like this:\n",
    "\n",
    "    DATASETS:\n",
    "      TRAIN: (\"coco_nuclei_train\", \"coco_nuclei_val\")\n",
    "      TEST: (\"coco_nuclei_test\",)\n",
    "\n",
    "Here, several training parameters may be specified and adjusted to the number of GPUs being used while training. These parameters are for training on ONE GPU:\n",
    "\n",
    "    SOLVER:\n",
    "      BASE_LR: 0.0025\n",
    "      STEPS: (480000, 640000)\n",
    "      MAX_ITER: 720000\n",
    "      IMS_PER_BATCH: 2\n",
    "\n",
    "The developer's recommmend multiplying the learning rate and the images per batch by the number of GPUs and dividing the steps and the maximum iterations by the number GPUs. See examples in maskrcnn-benchmark/configs/, where they were training with 8 GPUs. One may add the output directory as well with `OUTPUT_DIR: \"/path/to/\"`.\n",
    "\n",
    "I ran into an error `IndexError: index 0 is out of bounds for dimension 0 with size 0` while training on a test data set from labelbox with relatively small .jpeg images (I did not get the error while training the nuclei though). A quick fix can be setting `DATALOADER.ASPECT_RATIO_GROUPING = FALSE` in the config file, but the developpers are still checking the code and have not provided another solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run:\n",
    "\n",
    "    python maskrcnn-benchmark/tools/train_net.py --config-file \"path/to/config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When receiving Segmentation fault error, the GCC version may be updated (has to be version +4.9). Info here https://paper.dropbox.com/doc/Working-on-Monod-setup-environments-and-run-on-GPUs--AXH64wJuBgEe8XwCtJ09DZCqAg-hX2FfDYdlhY10ksm0BhH6. Checking the GCC version cna be done with `gcc --version`. After updating GCC to a higher version one has to recompile maskrcnn-benchmark by removing the folder `maskrcnn-benchmark/build` and do `python maskrcnn-benchmark/setup.py build develop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matterport_MaskRCNN",
   "language": "python",
   "name": "mat_mrcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
