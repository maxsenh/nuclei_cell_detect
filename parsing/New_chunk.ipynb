{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#import mrcnn\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "#from mrcnn.visualize import random_colors, apply_mask, find_contours\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import toimage, imsave\n",
    "from skimage import io\n",
    "import glob\n",
    "import skimage\n",
    "from skimage import measure\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import urllib.request\n",
    "from shapely.affinity import scale\n",
    "from skimage.color import grey2rgb\n",
    "#import torch\n",
    "import matplotlib.pylab as pylab\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "import maskrcnn_benchmark.structures.segmentation_mask\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"/home/maxsen/git/master_thesis/data/\"\n",
    "DATA = \"/home/maxsen/DEEPL/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions from matterport maskrcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download images to chunk them\n",
    "def download_img(anno_file, img_dir):\n",
    "    \n",
    "    anno = json.load(open(anno_file))\n",
    "    for s in anno:\n",
    "        name = s['External ID']\n",
    "        url = s['Labeled Data']\n",
    "        urllib.request.urlretrieve(url, img_dir + name)\n",
    "        print('downloading %s'%(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json file with segmented info and link to the cloud storage\n",
    "# with the images\n",
    "\n",
    "def labelbox_annotations_parser(annotations_dir,annotation_file,img_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Remember that for labelbox:\n",
    "    - origin (0,0) of the reference system in bottom left corner like\n",
    "    cartesian system\n",
    "    - labelbox can label objects outside the image so the coords\n",
    "    need to be trimmed to the image size\n",
    "    - This parser consider that all the images are square and same image size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    annotations = json.load(open(annotations_dir+annotation_file))\n",
    "    \n",
    "    # Parse the annotation of the images done using labelbox.io\n",
    "    # into a better formatted dict\n",
    "    # Check with the company  where is the origin\n",
    "\n",
    "    # create incremental number for all the identified objects\n",
    "    obj_idx = 0\n",
    "\n",
    "    print(annotations[0])\n",
    "    # create a labels list\n",
    "    labels_list = [] \n",
    "    labels_dict = {}\n",
    "\n",
    "    # Reformatted dict\n",
    "    annotations_dict = {}\n",
    "\n",
    "\n",
    "    # Masks reference\n",
    "    masks_urls_dict = {}\n",
    "\n",
    "    # Loop through all the images\n",
    "    # TODO: Check to skip non annotated images \n",
    "    for annotation_data in annotations:\n",
    "\n",
    "        # Avoid error when empty images\n",
    "        if isinstance(annotation_data['Label'],dict):\n",
    "        # save the image id\n",
    "            img_id = annotation_data['External ID']\n",
    "            annotations_dict[img_id] = {}\n",
    "            masks_urls_dict[img_id] = {}\n",
    "\n",
    "\n",
    "            # create subdict with labels\n",
    "            for lab in annotation_data['Label'].keys():\n",
    "                if lab not in labels_list:\n",
    "                    labels_list.append(lab) \n",
    "                annotations_dict[img_id][lab] = {}\n",
    "\n",
    "            # Create labels dict with reference number starting from 1\n",
    "            # because zero is for background\n",
    "            lab_idx = 1\n",
    "            for lab in labels_list:\n",
    "                labels_dict[lab] = lab_idx\n",
    "                lab_idx += 1\n",
    "\n",
    "            # loop through the labels and convert the coords in a list of tuples\n",
    "            for lab, objs in annotation_data['Label'].items():\n",
    "                for obj_coords in objs:\n",
    "                    coords = [tuple(coord_pair.values()) for coord_pair in obj_coords]\n",
    "                \n",
    "                    # Convert the coords in numpy array\n",
    "                    coords = np.array(coords)\n",
    "                    # labelbox reference system has (0,0) on the bottom left corner\n",
    "                    # need to correct in order to have skimage system\n",
    "                \n",
    "                    # Flip cartesian y-axis\n",
    "                    coords = np.abs(coords-np.array([0,img_size]))\n",
    "                \n",
    "                    # Swap x,y converting to [rr,cc] system\n",
    "                    coords[:,[0,1]] = coords[:,[1,0]]\n",
    "                \n",
    "                    # Remove values that are ouside the range\n",
    "                    #Remove points below 0\n",
    "                    coords[coords<0] = 0\n",
    "                \n",
    "                    # Remove the point out of the upper limit of the edges\n",
    "                    coords[coords>img_size] = img_size\n",
    "                \n",
    "                \n",
    "                    annotations_dict[img_id][lab][obj_idx] = coords\n",
    "                    obj_idx +=1\n",
    "\n",
    "        \n",
    "#        # Collect all the url for the masks\n",
    "#        for lab in annotation_data['Masks'].keys():\n",
    "#            masks_urls_dict[img_id][lab] = annotation_data['Masks'][lab]\n",
    "            \n",
    "    return annotations_dict, masks_urls_dict, labels_dict\n",
    "\n",
    "# chunking\n",
    "def chunking_labeled_images(number_chunks_dimension,class_names, img_raw,chunked_dir,annotations_dict):\n",
    "\n",
    "\n",
    "    # Get image ids from annotations\n",
    "    image_ids = list(annotations_dict.keys())\n",
    "\n",
    "    for image_id in image_ids:\n",
    "\n",
    "        # Define the image path\n",
    "        image_path = img_raw + image_id\n",
    "\n",
    "        # segmented regions\n",
    "        polygons = annotations_dict[image_id]\n",
    "\n",
    "\n",
    "        # Determine the size of the image\n",
    "        image = skimage.io.imread(image_path)\n",
    "        \n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        \n",
    "        # Convert the image to uint8\n",
    "        image = skimage.util.img_as_ubyte(image)\n",
    "        # image = skimage.color.grey2rgb(image)\n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        # Load and create masks array\n",
    "\n",
    "        # Determine how many objects of the different classes have been segmented\n",
    "        instance_count = 0\n",
    "        for lab in polygons:\n",
    "            instance_count += len(list(polygons[lab].values()))\n",
    "\n",
    "        mask = np.zeros([height, width, instance_count],\n",
    "                        dtype=np.uint8)\n",
    "\n",
    "        class_ids = []\n",
    "        mask_layer = 0\n",
    "        for lab, pgs in polygons.items():\n",
    "            \n",
    "            # Get the class id\n",
    "            class_id = class_names.index(lab)\n",
    "\n",
    "            for obj_num, coords in pgs.items():\n",
    "                \n",
    "                rr, cc = skimage.draw.polygon(coords[:,0], coords[:,1])\n",
    "                mask[rr, cc, mask_layer] = class_id\n",
    "                mask_layer += 1\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "\n",
    "        class_ids = np.array(class_ids)\n",
    "\n",
    "        # account for colored images\n",
    "        if image.shape[2] > 1:\n",
    "            print('ss')\n",
    "            dstack = np.dsplit(image,3)\n",
    "            stack1 = dstack[0][:, :, 0]\n",
    "            stack2 = dstack[1][:, :, 0]\n",
    "            stack3 = dstack[2][:, :, 0]\n",
    "            image_mask = np.insert(mask, 0, stack1, axis=2)\n",
    "            image_mask = np.insert(image_mask, 0, stack2, axis=2)\n",
    "            image_mask = np.insert(image_mask, 0, stack3, axis=2)\n",
    "        else:\n",
    "            image_mask = np.insert(mask,0,image,axis=2)\n",
    "        \n",
    "        print(image_mask.shape)\n",
    "        sstack = np.dsplit(image_mask, image_mask.shape[2])\n",
    "        plt.imshow(np.dstack((sstack[2], sstack[1], sstack[0])))\n",
    "        plt.show()\n",
    "        \n",
    "        # stack image with masks\n",
    "\n",
    "        # consider that we are collecting square images and only even number of cuts\n",
    "        hsplits = np.split(image_mask,number_chunks_dimension,axis=0)\n",
    "        total_images = []\n",
    "        for split in hsplits:\n",
    "                total_images.append(np.split(split,number_chunks_dimension,axis=1))\n",
    "        total_images = [img for cpl in total_images for img in cpl] \n",
    "\n",
    "        # Chunk the images and the segmented data\n",
    "        for idx,image_chunk in enumerate(total_images):\n",
    "            image_chunks_ids = []\n",
    "            mask = image_chunk != 0\n",
    "            planes_to_keep = np.flatnonzero((mask).sum(axis=(0,1)))\n",
    "            # Make sure that the image has labeled objects\n",
    "            if planes_to_keep.size:\n",
    "                image_chunk_trimmed = image_chunk[:,:,planes_to_keep]\n",
    "                image_chunk_trimmed_id = image_id.split('.')[0]+'chunk'+str(idx)\n",
    "\n",
    "                np.save(chunked_dir + image_chunk_trimmed_id, image_chunk_trimmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumber_chunks_dimension = 2\\nclass_names = [\\'BG\\']\\n_ = [class_names.append(i) for i in labels_dict]\\n#print(class_names)\\n\\n# use this if using personal pc\\n#ROOT = \"/home/maxsen/git/master_thesis/data/\"\\n#DATA = \"/home/maxsen/DEEPL/data/\"\\n\\n# dir to save the chunks\\nchunks_to_save_dir = DATA + \\'20190306_poly_t/npy/\\'\\n#chunks_to_save_dir = DATA + \\'nuclei_20181107_data/what_npy/\\'\\n\\n# run chunking\\n\\nfor s in os.listdir(raw_images):\\n    img = Image.open(raw_images + s)\\n    print(img)\\n    plt.imshow(img)\\n    plt.show()\\n\\nchunking_labeled_images(number_chunks_dimension,\\n                        class_names, raw_images,\\n                        chunks_to_save_dir,\\n                        annotations_dict)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT = \"/home/max/github/master_thesis/data/\"\n",
    "DATA = \"/data/proj/smFISH/Students/Max_Senftleben/files/data/\"\n",
    "#DATA = \"/home/maxsen/DEEPL/data/training_data/\"\n",
    "\n",
    "raw_images = DATA + '20190306_poly_t/raw/'\n",
    "#raw_images = DATA + 'raw/raw_nuclei/'\n",
    "\n",
    "anno_dir = '/home/max/github/ms2/data/annotations/'\n",
    "#print(os.listdir(anno_dir))\n",
    "\n",
    "start_annotation_file = 'export-2019-03-11T16_23_23.975Z_coco.json'\n",
    "#start_annotation_file = 'All_labels_20181107.json'\n",
    "\n",
    "# download images\n",
    "#download_img(anno_dir + start_annotation_file, raw_images)\n",
    "\n",
    "# transform image names to coco format\n",
    "#trans(raw_images, anno_dir + start_annotation_file)\n",
    "\n",
    "\n",
    "# parse annotation file\n",
    "img_size = 1024\n",
    "\n",
    "\n",
    "#annotations_dict, masks_urls_dict, labels_dict = labelbox_annotations_parser(anno_dir, \n",
    "#                                                                             start_annotation_file, \n",
    "#                                                                             img_size)\n",
    "# Number of chunks to create\n",
    "# 2x2 or 4x4 etc.....\n",
    "# it must be even\n",
    "'''\n",
    "number_chunks_dimension = 2\n",
    "class_names = ['BG']\n",
    "_ = [class_names.append(i) for i in labels_dict]\n",
    "#print(class_names)\n",
    "\n",
    "# use this if using personal pc\n",
    "#ROOT = \"/home/maxsen/git/master_thesis/data/\"\n",
    "#DATA = \"/home/maxsen/DEEPL/data/\"\n",
    "\n",
    "# dir to save the chunks\n",
    "chunks_to_save_dir = DATA + '20190306_poly_t/npy/'\n",
    "#chunks_to_save_dir = DATA + 'nuclei_20181107_data/what_npy/'\n",
    "\n",
    "# run chunking\n",
    "\n",
    "for s in os.listdir(raw_images):\n",
    "    img = Image.open(raw_images + s)\n",
    "    print(img)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "chunking_labeled_images(number_chunks_dimension,\n",
    "                        class_names, raw_images,\n",
    "                        chunks_to_save_dir,\n",
    "                        annotations_dict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sub mask\n",
    "def make_sub_masks(np_array):\n",
    "\n",
    "    \n",
    "    sub_masks = {}\n",
    "    \n",
    "    width, height, dim = np_array.shape\n",
    "    \n",
    "    # go through the np.array pixel-wise\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            \n",
    "            # Get the pixel values from the numpy array\n",
    "            a = np_array[y,x,0]\n",
    "            b = np_array[y,x,1]\n",
    "            c = np_array[y,x,2]\n",
    "            # get pixel in RGB format\n",
    "            pixel = (a,b,c)\n",
    "            \n",
    "            # if pixel is not background/black\n",
    "            if pixel != (0,0,0):\n",
    "                pixel_str = str(pixel)\n",
    "                sub_mask = sub_masks.get(pixel_str)\n",
    "                \n",
    "                if sub_mask is None:\n",
    "                    \n",
    "                    # Create a sub-mask (one bit per pixel) and add to the dictionary\n",
    "                    # Note: we add 1 pixel of padding in each direction\n",
    "                    # because the contours module doesn't handle cases\n",
    "                    # where pixels bleed to the edge of the image\n",
    "                    \n",
    "                    sub_masks[pixel_str] = Image.new('1', (width+2, height+2))\n",
    "\n",
    "                # Set the pixel value to 1 (default is 0), accounting for padding\n",
    "                sub_masks[pixel_str].putpixel((x+1, y+1), 1)\n",
    "    \n",
    "    return sub_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make annotation for mask\n",
    "def create_sub_mask_annotation(sub_mask, image_id, category_id, annotation_id, is_crowd = 0):\n",
    "    \n",
    "    # Find contours (boundary lines) around each sub-mask\n",
    "    # Note: there could be multiple contours if the object\n",
    "    # is partially occluded. (E.g. an elephant behind a tree)\n",
    "    contours = measure.find_contours(sub_mask, 0.5, positive_orientation = \"low\")\n",
    "    \n",
    "    segmentations = []\n",
    "    polygons = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Flip from (row, col) representation to (x, y)\n",
    "        # and subtract the padding pixel\n",
    "        for i in range(len(contour)):\n",
    "            row, col = contour[i]\n",
    "            contour[i] = (col - 1, row - 1)\n",
    "\n",
    "        # Make Polygon and simplify it\n",
    "        \n",
    "        #contour = contour.dot([[-1,0],[0,-1]])\n",
    "        #contour = abs(contour)\n",
    "        poly = Polygon(contour)\n",
    "        \n",
    "        poly = poly.simplify(1.0, preserve_topology = False)\n",
    "        \n",
    "        # transform IMPORTANT\n",
    "        \n",
    "        \n",
    "        polygons.append(poly)\n",
    "        \n",
    "        #print(\"contour: \", contour)\n",
    "        #print(\"poly.wkt: \", poly.wkt)\n",
    "        \n",
    "        segmentation = np.array(poly.exterior.coords).ravel().tolist()\n",
    "        segmentations.append(segmentation)\n",
    "        \n",
    "    # combine polygons to calculate the bounding box and area\n",
    "    multi_poly = MultiPolygon(polygons)\n",
    "    x, y, max_x, max_y = multi_poly.bounds\n",
    "    width = max_x - x\n",
    "    height = max_y - y\n",
    "    bbox = (x, y, width, height)\n",
    "    area = multi_poly.area\n",
    "\n",
    "    # write everything in coco-annotation-\n",
    "    # like format\n",
    "    annotation = {\n",
    "        'segmentation': segmentations,\n",
    "        'iscrowd': is_crowd,\n",
    "        'image_id': int(image_id),\n",
    "        'category_id': int(category_id),\n",
    "        'id': int(annotation_id),\n",
    "        'bbox': bbox,\n",
    "        'area': area\n",
    "    }\n",
    "    \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks_boxes(np_array, filename, width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0, augment = True):\n",
    "        \n",
    "        if augment == True:\n",
    "            chunk_id = str(filename[-len(\"00.npy\"):-len(\".npy\")])\n",
    "\n",
    "            # depending on dataset used        \n",
    "            if filename.startswith('Nuclei_SN'):\n",
    "                img_id = str(filename[len(\"Nuclei_SN_Hyb2_pos_\"):-len(\"chunk00.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"Nuclei_SN_Hyb2_pos_\")] + str(image_id) + \".png\"\n",
    "\n",
    "            elif filename.startswith('Raw_Nuclei'):\n",
    "                img_id = str(filename[len(\"Raw_Nuclei_\"):-len(\"chunk00.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"Raw_Nuclei_\")] + str(image_id) + \".png\"\n",
    "\n",
    "            elif filename.startswith('B'):\n",
    "                img_id = str(filename[len(\"BE87_\"):-len(\"chunk00.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"BE87_\")] + str(image_id) + \".png\"\n",
    "        else:\n",
    "            chunk_id = str(filename[-len(\"0.npy\"):-len(\".npy\")])\n",
    "\n",
    "            # depending on dataset used        \n",
    "            if filename.startswith('Nuclei_SN'):\n",
    "                img_id = str(filename[len(\"Nuclei_SN_Hyb2_pos_\"):-len(\"chunk0.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"Nuclei_SN_Hyb2_pos_\")] + str(image_id) + \".png\"\n",
    "\n",
    "            elif filename.startswith('Raw_Nuclei'):\n",
    "                img_id = str(filename[len(\"Raw_Nuclei_\"):-len(\"chunk0.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"Raw_Nuclei_\")] + str(image_id) + \".png\"\n",
    "\n",
    "            elif filename.startswith('B'):\n",
    "                img_id = str(filename[len(\"BE87_\"):-len(\"chunk0.npy\")])\n",
    "                image_id = (img_id + chunk_id)\n",
    "                filename = filename[0:len(\"BE87_\")] + str(image_id) + \".png\"\n",
    "        # full image_id, i.e. 150 see above\n",
    "        array_split = np.split(np_array, dim, axis=2)\n",
    "        \n",
    "        # images without segmentations have shape (1024, 1024, 1)\n",
    "        # updated filename with correct image_id\n",
    "        \n",
    "        annotations = []\n",
    "        \n",
    "        \n",
    "        # account for non-empty annotation\n",
    "        if True:\n",
    "            image = {\n",
    "                'id' : int(image_id),\n",
    "                'width' : width,\n",
    "                'height' : height,\n",
    "                'file_name' : filename,\n",
    "                'license' : None,\n",
    "                'flickr_url' : None,\n",
    "                'coco_url' : None,\n",
    "                'date_captured' : None\n",
    "            }\n",
    "            \n",
    "            #images.append(image)\n",
    "            \n",
    "            # save ground truth\n",
    "            #print(len(array_split))\n",
    "            \n",
    "            # use this when handling colored images\n",
    "            #ground_truth = np.dstack((array_split[0],array_split[0], array_split[0]))\n",
    "\n",
    "            ground_truth = grey2rgb(np.squeeze(array_split[0], axis = 2))\n",
    "            plt.imshow(ground_truth)\n",
    "            plt.show()\n",
    "            im = Image.fromarray(ground_truth)\n",
    "            im.save(images_to_save_folder + filename)\n",
    "            \n",
    "            # change to 3 if colored\n",
    "            for i in range(1, len(array_split)):\n",
    "                try:\n",
    "                    stacked_array = np.dstack((array_split[i], array_split[i], array_split[i]))\n",
    "                    sub_masks = make_sub_masks(stacked_array)\n",
    "                    \n",
    "                    for color, sub_mask in sub_masks.items():\n",
    "                        #print(category_ids)\n",
    "                        cat_id = category_ids[color]\n",
    "                        #print(cat_id)\n",
    "                        annotation = create_sub_mask_annotation(sub_mask, image_id, cat_id, annotation_id, is_crowd)\n",
    "                        annotations.append(annotation)\n",
    "                        #print(annotation_id)\n",
    "                        annotation_id += 1\n",
    "                except AttributeError:\n",
    "                    print(\"Attribute Error was raised, check shape of file\")\n",
    "            return image, annotations, annotation_id\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both scripts above\n",
    "def combine(categories, info, licenses, \n",
    "            category_ids, annotation_file, \n",
    "            chunks_dir,\n",
    "            images_to_save_folder,\n",
    "            is_crowd = 0, limit = None, AUGMENT = False):\n",
    "    \n",
    "    # set annotations and images\n",
    "    annotations = []\n",
    "    images = []\n",
    "    annotation_id = 1\n",
    "    \n",
    "    # go through folder of chunks in .npy format\n",
    "    # i.e. Nuclei_SN_Hyb2_pos_15chunk0.npy\n",
    "    # IMPORTANT\n",
    "    # the image_id will be 150, 15 for the image number\n",
    "    # and 0 for the first chunk of the whole image\n",
    "    # this is because the image_id has to be integer\n",
    "    # chunks were created like this:  0 1\n",
    "    #                                 2 3\n",
    "\n",
    "    # option to limit\n",
    "    x = 0\n",
    "    \n",
    "    for one_file in os.listdir(chunks_dir):\n",
    "        if limit:\n",
    "            if x <= limit:\n",
    "                print('pass')\n",
    "                pass\n",
    "            else:\n",
    "                print('continue')\n",
    "                continue\n",
    "                \n",
    "\n",
    "        print(\"File {} is processed\".format(one_file))\n",
    "        filename = one_file\n",
    "        print(filename[:-4] + '0.npy')\n",
    "        \n",
    "        # load numpy file\n",
    "        #print(chunks_dir + one_file)\n",
    "        np_array = np.load(chunks_dir + one_file)\n",
    "        height, width, dim = np_array.shape\n",
    "        \n",
    "        # do augmentation\n",
    "        if AUGMENT == True and dim > 2:    # numpy.rot90(m, k=1, axes=(0, 1))\n",
    "            \n",
    "            # offline augmentation\n",
    "            np_array1 = np.rot90(np_array, k=1, axes=(0, 1))\n",
    "            np_array2 = np.rot90(np_array, k=2, axes=(0, 1))\n",
    "            np_array3 = np.rot90(np_array, k=3, axes=(0, 1))\n",
    "            np_array4 = np.flip(np_array, axis=0)\n",
    "            np_array5 = np.flip(np_array1, axis=0)\n",
    "            np_array6 = np.flip(np_array2, axis=0)\n",
    "            np_array7 = np.flip(np_array3, axis=0)    \n",
    "            \n",
    "            #np_array, filename, width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0, augment = True)\n",
    "            \n",
    "            image, annos, anno_id = combine_masks_boxes(np_array, filename[:-4] + '0.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array1, filename[:-4] + '1.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array2, filename[:-4] + '2.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array3, filename[:-4] + '3.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array4, filename[:-4] + '4.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array5, filename[:-4] + '5.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array6, filename[:-4] + '6.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array7, filename[:-4] + '7.npy', width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            x += 1\n",
    "            \n",
    "        elif AUGMENT == False and dim > 1:\n",
    "            image, annos, anno_id = combine_masks_boxes(np_array, filename, width, height, dim, images_to_save_folder, annotation_id, category_ids, is_crowd = 0, augment = False)\n",
    "            annotation_id = anno_id\n",
    "            images.append(image)\n",
    "            annotations.append(annos)\n",
    "            print(\"Masks and Annotations obtained\")\n",
    "            x += 1\n",
    "        else:\n",
    "            print('No annos')\n",
    "        \n",
    "    # save all annotations to json file\n",
    "    print(\"write annotation to: \", annotation_file)\n",
    "    with open(annotation_file, \"w\") as write_file:\n",
    "        json.dump(\n",
    "                {\"info\":info,\n",
    "                 \"annotations\":[item for sublist in annotations for item in sublist], \n",
    "                 \"images\":images,\n",
    "                 \"licenses\":licenses,\n",
    "                 \"categories\":categories}, write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform image names to numbers so one can convert to COCO-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans(image_folder, anno_file):\n",
    "    files = os.listdir(image_folder)\n",
    "    anno = json.load(open(anno_file))\n",
    "    dict_trans = {name: name[2:6] + '_' + str(i) for i, name in enumerate(files, 40)}\n",
    "    [print(i, dict_trans[i]) for i in dict_trans]\n",
    "    for name in dict_trans:\n",
    "        new = dict_trans[name] + '.png'\n",
    "        os.rename(image_folder + name, image_folder + new)\n",
    "    for i, s in enumerate(anno):\n",
    "        if s['External ID'] in dict_trans:\n",
    "            anno[i]['External ID'] = dict_trans[anno[i]['External ID']] + '.png'\n",
    "    with open(anno_file[:-5] + '_coco.json', 'w') as w:\n",
    "        json.dump(anno, w)\n",
    "    # print conversion dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set variables and paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can be optimized much more, just a preliminary solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 is for nuclei, 2 is for undefined and 3 is for cluster\n",
    "category_ids_polyt = {\n",
    "                '(1, 1, 1)':1,\n",
    "                '(2, 2, 2)':3,\n",
    "                '(3, 3, 3)':2,\n",
    "                '(4, 4, 4)':4\n",
    "               }\n",
    "category_ids_nuclei = {\n",
    "                '(1, 1, 1)':1,\n",
    "                '(2, 2, 2)':3,\n",
    "                '(3, 3, 3)':2\n",
    "                }\n",
    "\n",
    "# pass categories in coco json file\n",
    "\n",
    "categories_nuclei = [{'supercategory': 'nuclei', 'id': 1, 'name': 'nuclei'}, \n",
    "              {'supercategory': 'undefined', 'id': 2, 'name': 'undefined'}, \n",
    "              {'supercategory': 'clusters', 'id': 3, 'name': 'clusters'}]\n",
    "\n",
    "categories_polyt = [\n",
    "    {'supercategory': 'Cell', 'id': 1, 'name': 'Nclei'}, \n",
    "    {'supercategory': 'Undefined', 'id': 3, 'name': 'Undefined'}, \n",
    "    {'supercategory': 'Clusters', 'id': 2, 'name': 'Clusters'},\n",
    "    {'supercategory': 'Junk', 'id': 4, 'name': 'Junk'}\n",
    "]\n",
    "\n",
    "# pass licenses\n",
    "licenses = []\n",
    "\n",
    "\n",
    "\n",
    "# pass info\n",
    "info = {'year': 2018, \n",
    "        'version': None, \n",
    "        'description': 'RNA staining segmentation SV', \n",
    "        'contributor': 'katharina.koetter@stud.ki.se', \n",
    "        'url': 'labelbox.com', \n",
    "        'date_created': '2018-04-12T09:46:52.000Z'\n",
    "       }\n",
    "\n",
    "#ROOT = \"/home/maxsen/git/master_thesis/data/\"\n",
    "#DATA = \"/home/maxsen/DEEPL/data/\"\n",
    "\n",
    "# dir of chunks in .npy format\n",
    "#CHUNKS = DATA + 'training_data/large_data_extracted_396chunks_2+2/'\n",
    "CHUNKS1 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/train_npy/'\n",
    "CHUNKS2 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/val_npy/'\n",
    "CHUNKS3 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/test_npy/'\n",
    "CHUNKS = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190309_aug_pop/test_npy/'\n",
    "# chunked images to save\n",
    "NEW_CHUNK_DIR1 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/train/'\n",
    "NEW_CHUNK_DIR2 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/val/'\n",
    "NEW_CHUNK_DIR3 = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190306_poly_t/test/'\n",
    "NEW_CHUNK_DIR = '/data/proj/smFISH/Students/Max_Senftleben/files/data/20190309_aug_pop/test/'\n",
    "# define annotation file\n",
    "anno_file1 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190306_poly_t/train_poly_t.json'\n",
    "anno_file2 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190306_poly_t/val_poly_t.json'\n",
    "anno_file3 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190306_poly_t/test_poly_t.json'\n",
    "anno_file = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190309_aug_pop/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the categories, ingo, licenses, \n",
    "# category_ids referring to the RGB-color-coded sub-classes,\n",
    "# annotation file, \n",
    "# directory of chunks to be processed,\n",
    "# directory of images to be saved\n",
    "\n",
    "\n",
    "def do():\n",
    "    combine(categories_nuclei, info, licenses, \n",
    "        category_ids_nuclei, anno_file, \n",
    "        CHUNKS, NEW_CHUNK_DIR, AUGMENT = False)\n",
    "    print('done')\n",
    "\n",
    "#do()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, result, save_path=None):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.imshow(img)\n",
    "    plt.axis('off')\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.imshow(result)\n",
    "    plt.axis('off')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### check chunks\n",
    "# increasing the size of figures\n",
    "pylab.rcParams['figure.figsize'] = 20, 12\n",
    "\n",
    "anno = anno_file\n",
    "img_dir = NEW_CHUNK_DIR\n",
    "import cv2\n",
    "\n",
    "'''coco_id = 1163\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "coco = COCO(anno_file)\n",
    "'''\n",
    "import maskrcnn_benchmark\n",
    "from maskrcnn_benchmark.structures.segmentation_mask import SegmentationMask\n",
    "from matplotlib.patches import Rectangle\n",
    "from pycocotools.coco import *\n",
    "import torch\n",
    "\n",
    "def check_chunks(anno):\n",
    "    anno = anno\n",
    "    data = json.load(open(anno))\n",
    "    sett = []\n",
    "    coco = COCO(anno) ; x = 0\n",
    "    for i,image in enumerate(data['images']):\n",
    "        if x >= 0:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "        x += 1\n",
    "\n",
    "\n",
    "        img = Image.open(img_dir + image['file_name'])\n",
    "        print(image['file_name'])\n",
    "        anno = [obj for obj in data['annotations'] if obj['image_id'] == image['id']]\n",
    "        boxes = [obj['bbox'] for obj in anno]\n",
    "        boxes = torch.as_tensor(boxes).reshape(-1,4)\n",
    "        target = BoxList(boxes, img.size, mode = 'xywh').convert('xyxy')\n",
    "        #print(image['file_name'], len(anno))\n",
    "        classes = [obj['category_id'] for obj in data['annotations'] if obj['image_id'] == image['id']]\n",
    "        json_category_id_to_contiguous_id = {\n",
    "                v: i + 1 for i, v in enumerate(coco.getCatIds())\n",
    "        }\n",
    "        classes = [json_category_id_to_contiguous_id[c] for c in classes]\n",
    "        classes = torch.tensor(classes)\n",
    "\n",
    "        target.add_field('labels', classes)\n",
    "        sett.append(len(anno))\n",
    "        #continue\n",
    "        masks = [obj[\"segmentation\"] for obj in anno]\n",
    "        masks = SegmentationMask(masks, img.size)\n",
    "        target.add_field(\"masks\", masks)\n",
    "\n",
    "        target = target.clip_to_image(remove_empty=True)\n",
    "\n",
    "\n",
    "\n",
    "        polygons = []\n",
    "        color = []\n",
    "        boxes = []\n",
    "\n",
    "        polys = vars(target)['extra_fields']['masks']\n",
    "        for polygon in polys:\n",
    "            try:\n",
    "                tenso = vars(polygon)['polygons'][0]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n",
    "            poly1 = tenso.numpy()\n",
    "            poly = poly1.reshape((int(len(poly1)/2),2))\n",
    "            polygons.append(Polygon(poly))\n",
    "            color.append(c)\n",
    "\n",
    "        xywh_tar = target.convert(\"xywh\")\n",
    "        for box in vars(xywh_tar)['bbox'].numpy():\n",
    "\n",
    "            rect = Rectangle((box[0],box[1]), box[2], box[3])\n",
    "            boxes.append(rect)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(1,2,1)\n",
    "        ax1.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "        ax2 = fig.add_subplot(1,2,2)\n",
    "        plt.imshow(img); plt.axis('off')\n",
    "\n",
    "        #print(polygons)\n",
    "        b = PatchCollection(boxes, facecolor = 'none', linewidths = 1, edgecolor = color)\n",
    "        p = PatchCollection(polygons, facecolor = 'none', linewidths = 0, alpha = 0.4)\n",
    "        ax2.add_collection(p)\n",
    "        #ax2.add_collection(b)\n",
    "        p = PatchCollection(polygons, facecolor = 'none', edgecolors = color, linewidths = 2)\n",
    "        ax2.add_collection(p)\n",
    "\n",
    "        #plt.savefig('/home/max/github/nuclei_cell_detect/new_images/BG92_5127_labeled.png', bbox_inches = 'tight')\n",
    "        plt.show()\n",
    "    print(max(sett))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check_chunks(anno[:-5] + '_pop.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(anno))\n",
    "new_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727]\n"
     ]
    }
   ],
   "source": [
    "# pop keys from files which are not \n",
    "# that great refer to log\n",
    "\n",
    "\n",
    "\n",
    "def pop_image_from_anno(anno_old, anno_new, list_to_pop):\n",
    "    data = json.load(open(anno_old))\n",
    "    to_write_imgs = [i for i in data['images'] if i['id'] not in list_to_pop]\n",
    "    to_write_anno = [i for i in data['annotations'] if i['image_id'] not in list_to_pop]\n",
    "    for x, i in enumerate(to_write_imgs):\n",
    "        #print(i)\n",
    "        if i['file_name'] == \"Raw_Nuclei_533.png\":\n",
    "            to_write_imgs[x]['id'] = 534\n",
    "            to_write_imgs[x]['file_name'] = 'Raw_Nuclei_534.png'\n",
    "            \n",
    "    for x, i in enumerate(to_write_anno):\n",
    "        if i['image_id'] == 533:\n",
    "            to_write_anno[x]['image_id'] = 534\n",
    "            \n",
    "    \n",
    "    with open(anno_new, 'w') as write:\n",
    "        json.dump(\n",
    "                {\"info\":data['info'],\n",
    "                 \"annotations\":to_write_anno, \n",
    "                 \"images\":to_write_imgs,\n",
    "                 \"licenses\":data['licenses'],\n",
    "                 \"categories\":data['categories']}, write)\n",
    "#list_1 = [1203, 1241, 1242, 1273, 1341, 1363, 1410, 1411, 1412, 1531, 1532, 1533, 152, 160, 162, 1861, 1883, 2120, 2122, 2150, 2172, 2180, 2181, 40]\n",
    "list_2 = [291, 311, 312, 332, 372]\n",
    "list_aug = []\n",
    "for i in list_2:\n",
    "    for s in range(8):\n",
    "        c = int(str(i) + str(s))\n",
    "        list_aug.append(c)\n",
    "print(list_aug)\n",
    "list_3 = [453, 503, 592, 600, 620, 712, 452, 500, 502, 503, 512, 552, 563, 600, 732, 761]\n",
    "list_4 = [452, 453,  483, 500, 502, 503, 510, 511, 512, 513, 562, 563, \n",
    "          581, 583, 590, 592, 600, 620, 622, 761, 763, 534, 533]\n",
    "anno_file = anno\n",
    "\n",
    "pop_image_from_anno(anno_file, anno_file[:-5] + '_pop.json', list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine annotation files\n",
    "def combine(anno_1, anno_2, anno_comb):\n",
    "    data1 = json.load(open(anno_1))\n",
    "    data2 = json.load(open(anno_2))\n",
    "    img = sum([data1['images'], data2['images']], [])\n",
    "    annos = sum([data1['annotations'], data2['annotations']], [])\n",
    "    \n",
    "    with open(anno_comb, 'w') as write:\n",
    "        json.dump(\n",
    "                {\"info\":data1['info'],\n",
    "                 \"annotations\":annos, \n",
    "                 \"images\":img,\n",
    "                 \"licenses\":data1['licenses'],\n",
    "                 \"categories\":data1['categories']}, write)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_file1 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_all_nuclei_large/train_nuclei_pop.json'\n",
    "anno_file2 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_all_nuclei_large/tval_nuclei_pop.json'\n",
    "anno_file3 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_all_nuclei_large/test_nuclei_pop.json'\n",
    "\n",
    "anno_1 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_raw_nuclei/train_nuclei.json'\n",
    "anno_2 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_raw_nuclei/val_nuclei.json'\n",
    "anno_3 = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190304_raw_nuclei/test_nuclei.json'\n",
    "\n",
    "new_anno = '/data/proj/smFISH/Students/Max_Senftleben/files/annotation/20190306_all_and_raw/'\n",
    "\n",
    "#combine(anno_file3, anno_3, new_anno + 'test_comb.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "mrcnn_b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
